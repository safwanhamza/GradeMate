{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc7fbfa9-d232-45f5-beae-540d9b4700a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\n",
      "---------------------------------- -----------\n",
      "alembic                            1.14.0\n",
      "annotated-types                    0.7.0\n",
      "anyio                              4.7.0\n",
      "assemblyai                         0.36.0\n",
      "asttokens                          3.0.0\n",
      "blinker                            1.9.0\n",
      "cachetools                         5.5.0\n",
      "certifi                            2024.12.14\n",
      "charset-normalizer                 3.4.0\n",
      "click                              8.1.8\n",
      "cloudpickle                        3.1.0\n",
      "colorama                           0.4.6\n",
      "comm                               0.2.2\n",
      "contourpy                          1.3.1\n",
      "cycler                             0.12.1\n",
      "databricks-sdk                     0.40.0\n",
      "debugpy                            1.8.11\n",
      "decorator                          5.1.1\n",
      "Deprecated                         1.2.15\n",
      "distlib                            0.3.9\n",
      "distro                             1.9.0\n",
      "docker                             7.1.0\n",
      "executing                          2.1.0\n",
      "filelock                           3.16.1\n",
      "Flask                              3.1.0\n",
      "fonttools                          4.55.3\n",
      "gitdb                              4.0.11\n",
      "GitPython                          3.1.43\n",
      "google-api-core                    2.24.0\n",
      "google-api-python-client           2.156.0\n",
      "google-auth                        2.37.0\n",
      "google-auth-httplib2               0.2.0\n",
      "googleapis-common-protos           1.66.0\n",
      "graphene                           3.4.3\n",
      "graphql-core                       3.2.5\n",
      "graphql-relay                      3.2.0\n",
      "h11                                0.14.0\n",
      "httpcore                           1.0.7\n",
      "httplib2                           0.22.0\n",
      "httpx                              0.28.1\n",
      "idna                               3.10\n",
      "importlib_metadata                 8.5.0\n",
      "ipykernel                          6.29.5\n",
      "ipython                            8.31.0\n",
      "itsdangerous                       2.2.0\n",
      "jedi                               0.19.2\n",
      "Jinja2                             3.1.5\n",
      "jiter                              0.8.2\n",
      "joblib                             1.4.2\n",
      "jupyter_client                     8.6.3\n",
      "jupyter_core                       5.7.2\n",
      "kiwisolver                         1.4.7\n",
      "lxml                               5.3.0\n",
      "Mako                               1.3.8\n",
      "Markdown                           3.7\n",
      "MarkupSafe                         3.0.2\n",
      "matplotlib                         3.10.0\n",
      "matplotlib-inline                  0.1.7\n",
      "mlflow                             2.19.0\n",
      "mlflow-skinny                      2.19.0\n",
      "nest-asyncio                       1.6.0\n",
      "numpy                              2.2.1\n",
      "openai                             1.58.1\n",
      "opentelemetry-api                  1.29.0\n",
      "opentelemetry-sdk                  1.29.0\n",
      "opentelemetry-semantic-conventions 0.50b0\n",
      "packaging                          24.2\n",
      "pandas                             2.2.3\n",
      "pandoc                             2.4\n",
      "parso                              0.8.4\n",
      "patsy                              1.0.1\n",
      "pillow                             11.0.0\n",
      "pip                                24.3.1\n",
      "pipenv                             2024.4.0\n",
      "platformdirs                       4.3.6\n",
      "plumbum                            1.9.0\n",
      "ply                                3.11\n",
      "prompt_toolkit                     3.0.48\n",
      "proto-plus                         1.25.0\n",
      "protobuf                           5.29.2\n",
      "psutil                             6.1.1\n",
      "pure_eval                          0.2.3\n",
      "pyarrow                            18.1.0\n",
      "pyasn1                             0.6.1\n",
      "pyasn1_modules                     0.4.1\n",
      "pydantic                           2.10.4\n",
      "pydantic_core                      2.27.2\n",
      "Pygments                           2.18.0\n",
      "pypandoc                           1.14\n",
      "pyparsing                          3.2.0\n",
      "python-dateutil                    2.9.0.post0\n",
      "python-docx                        1.1.2\n",
      "pytz                               2024.2\n",
      "pywin32                            308\n",
      "PyYAML                             6.0.2\n",
      "pyzmq                              26.2.0\n",
      "requests                           2.32.3\n",
      "rsa                                4.9\n",
      "scikit-learn                       1.6.0\n",
      "scipy                              1.14.1\n",
      "seaborn                            0.13.2\n",
      "setuptools                         75.6.0\n",
      "six                                1.17.0\n",
      "smmap                              5.0.1\n",
      "sniffio                            1.3.1\n",
      "SQLAlchemy                         2.0.36\n",
      "sqlparse                           0.5.3\n",
      "stack-data                         0.6.3\n",
      "statsmodels                        0.14.4\n",
      "threadpoolctl                      3.5.0\n",
      "tornado                            6.4.2\n",
      "tqdm                               4.67.1\n",
      "traitlets                          5.14.3\n",
      "typing_extensions                  4.12.2\n",
      "tzdata                             2024.2\n",
      "uritemplate                        4.1.1\n",
      "urllib3                            2.3.0\n",
      "virtualenv                         20.28.0\n",
      "waitress                           3.0.2\n",
      "wcwidth                            0.2.13\n",
      "websockets                         14.1\n",
      "Werkzeug                           3.1.3\n",
      "wheel                              0.45.1\n",
      "wrapt                              1.17.0\n",
      "yt-dlp                             2024.12.23\n",
      "zipp                               3.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lflow (C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2ae15b-1adf-4048-9406-60336ab62c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\n",
      "---------------------------------- -----------\n",
      "alembic                            1.14.0\n",
      "annotated-types                    0.7.0\n",
      "anyio                              4.7.0\n",
      "assemblyai                         0.36.0\n",
      "asttokens                          3.0.0\n",
      "blinker                            1.9.0\n",
      "cachetools                         5.5.0\n",
      "certifi                            2024.12.14\n",
      "charset-normalizer                 3.4.0\n",
      "click                              8.1.8\n",
      "cloudpickle                        3.1.0\n",
      "colorama                           0.4.6\n",
      "comm                               0.2.2\n",
      "contourpy                          1.3.1\n",
      "cycler                             0.12.1\n",
      "databricks-sdk                     0.40.0\n",
      "debugpy                            1.8.11\n",
      "decorator                          5.1.1\n",
      "Deprecated                         1.2.15\n",
      "distlib                            0.3.9\n",
      "distro                             1.9.0\n",
      "docker                             7.1.0\n",
      "executing                          2.1.0\n",
      "filelock                           3.16.1\n",
      "Flask                              3.1.0\n",
      "fonttools                          4.55.3\n",
      "gitdb                              4.0.11\n",
      "GitPython                          3.1.43\n",
      "google-api-core                    2.24.0\n",
      "google-api-python-client           2.156.0\n",
      "google-auth                        2.37.0\n",
      "google-auth-httplib2               0.2.0\n",
      "googleapis-common-protos           1.66.0\n",
      "graphene                           3.4.3\n",
      "graphql-core                       3.2.5\n",
      "graphql-relay                      3.2.0\n",
      "h11                                0.14.0\n",
      "httpcore                           1.0.7\n",
      "httplib2                           0.22.0\n",
      "httpx                              0.28.1\n",
      "idna                               3.10\n",
      "importlib_metadata                 8.5.0\n",
      "ipykernel                          6.29.5\n",
      "ipython                            8.31.0\n",
      "itsdangerous                       2.2.0\n",
      "jedi                               0.19.2\n",
      "Jinja2                             3.1.5\n",
      "jiter                              0.8.2\n",
      "joblib                             1.4.2\n",
      "jupyter_client                     8.6.3\n",
      "jupyter_core                       5.7.2\n",
      "kiwisolver                         1.4.7\n",
      "lxml                               5.3.0\n",
      "Mako                               1.3.8\n",
      "Markdown                           3.7\n",
      "MarkupSafe                         3.0.2\n",
      "matplotlib                         3.10.0\n",
      "matplotlib-inline                  0.1.7\n",
      "mlflow                             2.19.0\n",
      "mlflow-skinny                      2.19.0\n",
      "nest-asyncio                       1.6.0\n",
      "numpy                              2.2.1\n",
      "openai                             1.58.1\n",
      "opentelemetry-api                  1.29.0\n",
      "opentelemetry-sdk                  1.29.0\n",
      "opentelemetry-semantic-conventions 0.50b0\n",
      "packaging                          24.2\n",
      "pandas                             2.2.3\n",
      "pandoc                             2.4\n",
      "parso                              0.8.4\n",
      "patsy                              1.0.1\n",
      "pillow                             11.0.0\n",
      "pip                                24.3.1\n",
      "pipenv                             2024.4.0\n",
      "platformdirs                       4.3.6\n",
      "plumbum                            1.9.0\n",
      "ply                                3.11\n",
      "prompt_toolkit                     3.0.48\n",
      "proto-plus                         1.25.0\n",
      "protobuf                           5.29.2\n",
      "psutil                             6.1.1\n",
      "pure_eval                          0.2.3\n",
      "pyarrow                            18.1.0\n",
      "pyasn1                             0.6.1\n",
      "pyasn1_modules                     0.4.1\n",
      "pydantic                           2.10.4\n",
      "pydantic_core                      2.27.2\n",
      "Pygments                           2.18.0\n",
      "pypandoc                           1.14\n",
      "pyparsing                          3.2.0\n",
      "python-dateutil                    2.9.0.post0\n",
      "python-docx                        1.1.2\n",
      "pytz                               2024.2\n",
      "pywin32                            308\n",
      "PyYAML                             6.0.2\n",
      "pyzmq                              26.2.0\n",
      "requests                           2.32.3\n",
      "rsa                                4.9\n",
      "scikit-learn                       1.6.0\n",
      "scipy                              1.14.1\n",
      "seaborn                            0.13.2\n",
      "setuptools                         75.6.0\n",
      "six                                1.17.0\n",
      "smmap                              5.0.1\n",
      "sniffio                            1.3.1\n",
      "SQLAlchemy                         2.0.36\n",
      "sqlparse                           0.5.3\n",
      "stack-data                         0.6.3\n",
      "statsmodels                        0.14.4\n",
      "threadpoolctl                      3.5.0\n",
      "tornado                            6.4.2\n",
      "tqdm                               4.67.1\n",
      "traitlets                          5.14.3\n",
      "typing_extensions                  4.12.2\n",
      "tzdata                             2024.2\n",
      "uritemplate                        4.1.1\n",
      "urllib3                            2.3.0\n",
      "virtualenv                         20.28.0\n",
      "waitress                           3.0.2\n",
      "wcwidth                            0.2.13\n",
      "websockets                         14.1\n",
      "Werkzeug                           3.1.3\n",
      "wheel                              0.45.1\n",
      "wrapt                              1.17.0\n",
      "yt-dlp                             2024.12.23\n",
      "zipp                               3.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lflow (C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768e31c-8ae4-4941-baf4-826ffc487924",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53408320-34e4-4a7b-8e97-e3eb083946cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer (e.g., BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def semantic_chunk(text, max_length=512):\n",
    "    \"\"\"\n",
    "    Splits text into chunks based on semantic boundaries using a language model.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in text.split(\".\"):\n",
    "        sentence_length = len(tokenizer.tokenize(sentence))\n",
    "        if current_length + sentence_length < max_length:\n",
    "            current_chunk += sentence + \".\"\n",
    "            current_length += sentence_length\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence + \".\"\n",
    "            current_length = sentence_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84014e4-7352-4eb6-a0fc-e631c99db06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def expand_query(query):\n",
    "    \"\"\"\n",
    "    Expands the query with synonyms using WordNet.\n",
    "    \"\"\"\n",
    "    expanded_query = []\n",
    "    for word in query.split():\n",
    "        synonyms = [syn.name() for syn in wordnet.synsets(word)]\n",
    "        expanded_query.extend(synonyms)\n",
    "    expanded_query.append(query)  # Include the original query\n",
    "    return \" \".join(expanded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6f4ed-3603-42fd-9e16-8c44a45db992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6169977-cdb3-4221-8e02-3f33333a361c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80cef5-5e5a-4e3d-bcb1-3346c547389d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b41846b2-f773-4762-88ac-fbad54f0efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model...\n",
      "User Query: Explain the concept of Retrieval-Augmented Generation.\n",
      "Executing RAG pipeline...\n",
      "Retrieving relevant chunks...\n",
      "Error during retrieval: list index out of range\n",
      "Q: Explain the concept of Retrieval-Augmented Generation.\n",
      "A: No relevant chunks retrieved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configure paths\n",
    "DATA_DIR = \"./data\"  # Directory with PDF files\n",
    "LLAMA_API_URL  = \"http://localhost:11434/api/v1/completion\"  # Llama API endpoint\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Load SentenceTransformer for embeddings\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize FAISS Index\n",
    "dimension = 384  # Embedding size for all-MiniLM-L6-v2\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Metadata store\n",
    "metadata_store = []\n",
    "\n",
    "### Step 1: Pre-Retrieval Optimization - Chunking ###\n",
    "def load_and_chunk_pdfs(data_dir, chunk_size, chunk_overlap):\n",
    "    \"\"\"\n",
    "    Load PDF files, extract text, and chunk them using a text splitter.\n",
    "    \"\"\"\n",
    "    print(\"Loading and chunking PDFs...\")\n",
    "    chunks = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            try:\n",
    "                reader = PdfReader(filepath)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text()\n",
    "                doc_chunks = splitter.split_text(text)\n",
    "                chunks.extend([(chunk, {\"source\": filename}) for chunk in doc_chunks])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "    \n",
    "    print(f\"Total chunks created: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "# Load and chunk PDFs\n",
    "#chunked_data = load_and_chunk_pdfs(DATA_DIR, CHUNK_SIZE, CHUNK_OVERLAP)  ############\n",
    "\n",
    "### Step 2: Pre-Retrieval Optimization - Embedding and Indexing ###\n",
    "def index_chunks(chunks, model, index, metadata_store):\n",
    "    \"\"\"\n",
    "    Generate embeddings for chunks and add them to the FAISS index along with metadata.\n",
    "    \"\"\"\n",
    "    print(\"Generating embeddings and indexing chunks...\")\n",
    "    embeddings = []\n",
    "    for idx, (chunk, metadata) in enumerate(chunks):\n",
    "        try:\n",
    "            embedding = model.encode(chunk)\n",
    "            embeddings.append(embedding)\n",
    "            metadata_store.append(metadata)\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Processed {idx} chunks...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding for chunk {idx}: {e}\")\n",
    "    index.add(np.array(embeddings).astype('float32'))\n",
    "    print(\"Indexing complete.\")\n",
    "\n",
    "# Index the chunks\n",
    "#index_chunks(chunked_data, embedding_model, faiss_index, metadata_store)   ####\n",
    "\n",
    "### Step 3: Retrieval Optimization ###\n",
    "def retrieve_relevant_chunks(query, model, index, metadata_store, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-k relevant chunks for a given query using FAISS index.\n",
    "    \"\"\"\n",
    "    print(\"Retrieving relevant chunks...\")\n",
    "    try:\n",
    "        query_embedding = model.encode(query)\n",
    "        distances, indices = index.search(np.array([query_embedding]).astype('float32'), top_k)\n",
    "        retrieved_chunks = [(chunked_data[i][0], metadata_store[i]) for i in indices[0]]\n",
    "        print(f\"Retrieved {len(retrieved_chunks)} chunks.\")\n",
    "        return retrieved_chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error during retrieval: {e}\")\n",
    "        return []\n",
    "\n",
    "### Step 4: Post-Retrieval Optimization - Context and Prompt Engineering ###\n",
    "def generate_context(retrieved_chunks):\n",
    "    \"\"\"\n",
    "    Generate a context string by concatenating retrieved chunks.\n",
    "    \"\"\"\n",
    "    print(\"Generating context from retrieved chunks...\")\n",
    "    return \" \".join([chunk for chunk, _ in retrieved_chunks])\n",
    "\n",
    "def format_prompt(context, query):\n",
    "    \"\"\"\n",
    "    Format the prompt with the retrieved context and user query.\n",
    "    \"\"\"\n",
    "    print(\"Formatting the prompt...\")\n",
    "    return f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Query:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "### Step 5: Generation with Llama ###\n",
    "def call_llama_api(prompt):\n",
    "    \"\"\"\n",
    "    Call the Llama model's API to generate a response based on the prompt.\n",
    "    \"\"\"\n",
    "    print(\"Calling Llama API...\")\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 256,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(LLAMA_API_URL, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        print(\"Llama API call successful.\")\n",
    "        return response.json()[\"choices\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Llama API: {e}\")\n",
    "        return \"Error generating response.\"\n",
    "\n",
    "### Step 6: RAG Pipeline ###\n",
    "def rag_pipeline(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline:\n",
    "    1. Retrieve relevant chunks.\n",
    "    2. Generate context.\n",
    "    3. Format prompt.\n",
    "    4. Generate response using Llama.\n",
    "    \"\"\"\n",
    "    print(\"Executing RAG pipeline...\")\n",
    "    # Retrieve relevant chunks\n",
    "    retrieved_chunks = retrieve_relevant_chunks(query, embedding_model, faiss_index, metadata_store, top_k)\n",
    "\n",
    "    if not retrieved_chunks:\n",
    "        return \"No relevant chunks retrieved.\"\n",
    "\n",
    "    # Generate context\n",
    "    context = generate_context(retrieved_chunks)\n",
    "\n",
    "    # Format prompt\n",
    "    prompt = format_prompt(context, query)\n",
    "\n",
    "    # Generate response using Llama\n",
    "    response = call_llama_api(prompt)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example Query\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"Explain the concept of Retrieval-Augmented Generation.\"\n",
    "    print(f\"User Query: {user_query}\")\n",
    "    answer = rag_pipeline(user_query)\n",
    "    print(f\"Q: {user_query}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80c22913-1b18-47de-b511-903fd55beb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model...\n",
      "Loading and chunking PDFs...\n",
      "Processing file: 3.1 complet_ctst_gpt.pdf\n",
      "Processing file: Assignment 01.pdf\n",
      "Processing file: BSDSf21M007_Assignment_1.pdf\n",
      "Processing file: Handout 1.1 (Lab Environment Setup).pdf\n",
      "Processing file: Handout 1.2 (Recap of OS with Linux).pdf\n",
      "Processing file: Handout 1.3 (Recap of InterNetworking Concepts with Linux).pdf\n",
      "Processing file: Handout 2.1 (Ethical Hacking Pentesting and Anonymity).pdf\n",
      "Processing file: Handout 2.10 (Web App Pen Testing - II).pdf\n",
      "Processing file: Handout 2.2 (Reconnaissance Info Gathering and OSINT) (1).pdf\n",
      "Processing file: Handout 2.3 (Scanning and Vulnerability Analysis - I).pdf\n",
      "Processing file: Handout 2.4 (Scanning and Vulnerability Analysis - II).pdf\n",
      "Processing file: Handout 2.5 (Exploitation and Gaining Access).pdf\n",
      "Processing file: Handout 2.6 (Generating your own Payloads).pdf\n",
      "Processing file: Handout 2.7 (Privilege Escalation).pdf\n",
      "Processing file: Handout 2.8 (Persistence and Removing Tracks).pdf\n",
      "Processing file: Handout 2.9 (Web App Pen Testing - I).pdf\n",
      "Processing file: Handout 3.1 (C Compilation Under the Hood and Vulnerabilities).pdf\n",
      "Processing file: Handout 3.2 (x86-64 Assembly and Reverse Engineering).pdf\n",
      "Processing file: Network Scanning Techniques.pdf\n",
      "Processing file: Overview of the Course.pdf\n",
      "Processing file: Week 09_ Select and Train a Model.pdf\n",
      "Processing file: Week 10_ Data Definition and Baseline.pdf\n",
      "Total chunks created: 1756\n",
      "Generating embeddings and indexing chunks...\n",
      "Processed 0 chunks...\n",
      "Processed 10 chunks...\n",
      "Processed 20 chunks...\n",
      "Processed 30 chunks...\n",
      "Processed 40 chunks...\n",
      "Processed 50 chunks...\n",
      "Processed 60 chunks...\n",
      "Processed 70 chunks...\n",
      "Processed 80 chunks...\n",
      "Processed 90 chunks...\n",
      "Processed 100 chunks...\n",
      "Processed 110 chunks...\n",
      "Processed 120 chunks...\n",
      "Processed 130 chunks...\n",
      "Processed 140 chunks...\n",
      "Processed 150 chunks...\n",
      "Processed 160 chunks...\n",
      "Processed 170 chunks...\n",
      "Processed 180 chunks...\n",
      "Processed 190 chunks...\n",
      "Processed 200 chunks...\n",
      "Processed 210 chunks...\n",
      "Processed 220 chunks...\n",
      "Processed 230 chunks...\n",
      "Processed 240 chunks...\n",
      "Processed 250 chunks...\n",
      "Processed 260 chunks...\n",
      "Processed 270 chunks...\n",
      "Processed 280 chunks...\n",
      "Processed 290 chunks...\n",
      "Processed 300 chunks...\n",
      "Processed 310 chunks...\n",
      "Processed 320 chunks...\n",
      "Processed 330 chunks...\n",
      "Processed 340 chunks...\n",
      "Processed 350 chunks...\n",
      "Processed 360 chunks...\n",
      "Processed 370 chunks...\n",
      "Processed 380 chunks...\n",
      "Processed 390 chunks...\n",
      "Processed 400 chunks...\n",
      "Processed 410 chunks...\n",
      "Processed 420 chunks...\n",
      "Processed 430 chunks...\n",
      "Processed 440 chunks...\n",
      "Processed 450 chunks...\n",
      "Processed 460 chunks...\n",
      "Processed 470 chunks...\n",
      "Processed 480 chunks...\n",
      "Processed 490 chunks...\n",
      "Processed 500 chunks...\n",
      "Processed 510 chunks...\n",
      "Processed 520 chunks...\n",
      "Processed 530 chunks...\n",
      "Processed 540 chunks...\n",
      "Processed 550 chunks...\n",
      "Processed 560 chunks...\n",
      "Processed 570 chunks...\n",
      "Processed 580 chunks...\n",
      "Processed 590 chunks...\n",
      "Processed 600 chunks...\n",
      "Processed 610 chunks...\n",
      "Processed 620 chunks...\n",
      "Processed 630 chunks...\n",
      "Processed 640 chunks...\n",
      "Processed 650 chunks...\n",
      "Processed 660 chunks...\n",
      "Processed 670 chunks...\n",
      "Processed 680 chunks...\n",
      "Processed 690 chunks...\n",
      "Processed 700 chunks...\n",
      "Processed 710 chunks...\n",
      "Processed 720 chunks...\n",
      "Processed 730 chunks...\n",
      "Processed 740 chunks...\n",
      "Processed 750 chunks...\n",
      "Processed 760 chunks...\n",
      "Processed 770 chunks...\n",
      "Processed 780 chunks...\n",
      "Processed 790 chunks...\n",
      "Processed 800 chunks...\n",
      "Processed 810 chunks...\n",
      "Processed 820 chunks...\n",
      "Processed 830 chunks...\n",
      "Processed 840 chunks...\n",
      "Processed 850 chunks...\n",
      "Processed 860 chunks...\n",
      "Processed 870 chunks...\n",
      "Processed 880 chunks...\n",
      "Processed 890 chunks...\n",
      "Processed 900 chunks...\n",
      "Processed 910 chunks...\n",
      "Processed 920 chunks...\n",
      "Processed 930 chunks...\n",
      "Processed 940 chunks...\n",
      "Processed 950 chunks...\n",
      "Processed 960 chunks...\n",
      "Processed 970 chunks...\n",
      "Processed 980 chunks...\n",
      "Processed 990 chunks...\n",
      "Processed 1000 chunks...\n",
      "Processed 1010 chunks...\n",
      "Processed 1020 chunks...\n",
      "Processed 1030 chunks...\n",
      "Processed 1040 chunks...\n",
      "Processed 1050 chunks...\n",
      "Processed 1060 chunks...\n",
      "Processed 1070 chunks...\n",
      "Processed 1080 chunks...\n",
      "Processed 1090 chunks...\n",
      "Processed 1100 chunks...\n",
      "Processed 1110 chunks...\n",
      "Processed 1120 chunks...\n",
      "Processed 1130 chunks...\n",
      "Processed 1140 chunks...\n",
      "Processed 1150 chunks...\n",
      "Processed 1160 chunks...\n",
      "Processed 1170 chunks...\n",
      "Processed 1180 chunks...\n",
      "Processed 1190 chunks...\n",
      "Processed 1200 chunks...\n",
      "Processed 1210 chunks...\n",
      "Processed 1220 chunks...\n",
      "Processed 1230 chunks...\n",
      "Processed 1240 chunks...\n",
      "Processed 1250 chunks...\n",
      "Processed 1260 chunks...\n",
      "Processed 1270 chunks...\n",
      "Processed 1280 chunks...\n",
      "Processed 1290 chunks...\n",
      "Processed 1300 chunks...\n",
      "Processed 1310 chunks...\n",
      "Processed 1320 chunks...\n",
      "Processed 1330 chunks...\n",
      "Processed 1340 chunks...\n",
      "Processed 1350 chunks...\n",
      "Processed 1360 chunks...\n",
      "Processed 1370 chunks...\n",
      "Processed 1380 chunks...\n",
      "Processed 1390 chunks...\n",
      "Processed 1400 chunks...\n",
      "Processed 1410 chunks...\n",
      "Processed 1420 chunks...\n",
      "Processed 1430 chunks...\n",
      "Processed 1440 chunks...\n",
      "Processed 1450 chunks...\n",
      "Processed 1460 chunks...\n",
      "Processed 1470 chunks...\n",
      "Processed 1480 chunks...\n",
      "Processed 1490 chunks...\n",
      "Processed 1500 chunks...\n",
      "Processed 1510 chunks...\n",
      "Processed 1520 chunks...\n",
      "Processed 1530 chunks...\n",
      "Processed 1540 chunks...\n",
      "Processed 1550 chunks...\n",
      "Processed 1560 chunks...\n",
      "Processed 1570 chunks...\n",
      "Processed 1580 chunks...\n",
      "Processed 1590 chunks...\n",
      "Processed 1600 chunks...\n",
      "Processed 1610 chunks...\n",
      "Processed 1620 chunks...\n",
      "Processed 1630 chunks...\n",
      "Processed 1640 chunks...\n",
      "Processed 1650 chunks...\n",
      "Processed 1660 chunks...\n",
      "Processed 1670 chunks...\n",
      "Processed 1680 chunks...\n",
      "Processed 1690 chunks...\n",
      "Processed 1700 chunks...\n",
      "Processed 1710 chunks...\n",
      "Processed 1720 chunks...\n",
      "Processed 1730 chunks...\n",
      "Processed 1740 chunks...\n",
      "Processed 1750 chunks...\n",
      "Indexing complete.\n",
      "User Query: what is information technology?\n",
      "Executing RAG pipeline...\n",
      "Retrieving relevant chunks...\n",
      "Retrieved 5 chunks.\n",
      "Generating context from retrieved chunks...\n",
      "Formatting the prompt...\n",
      "Calling Llama API...\n",
      "Error calling Llama API: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/v1/completion (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000013A0285F9A0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "Q: what is information technology?\n",
      "A: Error generating response.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ed64628-26b7-4fa8-8a2c-8b83f974398a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model...\n",
      "Loading and chunking PDFs...\n",
      "Processing file: 3.1 complet_ctst_gpt.pdf\n",
      "Processing file: Assignment 01.pdf\n",
      "Processing file: BSDSf21M007_Assignment_1.pdf\n",
      "Processing file: Handout 1.1 (Lab Environment Setup).pdf\n",
      "Processing file: Handout 1.2 (Recap of OS with Linux).pdf\n",
      "Processing file: Handout 1.3 (Recap of InterNetworking Concepts with Linux).pdf\n",
      "Processing file: Handout 2.1 (Ethical Hacking Pentesting and Anonymity).pdf\n",
      "Processing file: Handout 2.10 (Web App Pen Testing - II).pdf\n",
      "Processing file: Handout 2.2 (Reconnaissance Info Gathering and OSINT) (1).pdf\n",
      "Processing file: Handout 2.3 (Scanning and Vulnerability Analysis - I).pdf\n",
      "Processing file: Handout 2.4 (Scanning and Vulnerability Analysis - II).pdf\n",
      "Processing file: Handout 2.5 (Exploitation and Gaining Access).pdf\n",
      "Processing file: Handout 2.6 (Generating your own Payloads).pdf\n",
      "Processing file: Handout 2.7 (Privilege Escalation).pdf\n",
      "Processing file: Handout 2.8 (Persistence and Removing Tracks).pdf\n",
      "Processing file: Handout 2.9 (Web App Pen Testing - I).pdf\n",
      "Processing file: Handout 3.1 (C Compilation Under the Hood and Vulnerabilities).pdf\n",
      "Processing file: Handout 3.2 (x86-64 Assembly and Reverse Engineering).pdf\n",
      "Processing file: Network Scanning Techniques.pdf\n",
      "Processing file: Overview of the Course.pdf\n",
      "Processing file: Week 09_ Select and Train a Model.pdf\n",
      "Processing file: Week 10_ Data Definition and Baseline.pdf\n",
      "Total chunks created: 1756\n",
      "Generating embeddings and indexing chunks...\n",
      "Processed 0 chunks...\n",
      "Processed 10 chunks...\n",
      "Processed 20 chunks...\n",
      "Processed 30 chunks...\n",
      "Processed 40 chunks...\n",
      "Processed 50 chunks...\n",
      "Processed 60 chunks...\n",
      "Processed 70 chunks...\n",
      "Processed 80 chunks...\n",
      "Processed 90 chunks...\n",
      "Processed 100 chunks...\n",
      "Processed 110 chunks...\n",
      "Processed 120 chunks...\n",
      "Processed 130 chunks...\n",
      "Processed 140 chunks...\n",
      "Processed 150 chunks...\n",
      "Processed 160 chunks...\n",
      "Processed 170 chunks...\n",
      "Processed 180 chunks...\n",
      "Processed 190 chunks...\n",
      "Processed 200 chunks...\n",
      "Processed 210 chunks...\n",
      "Processed 220 chunks...\n",
      "Processed 230 chunks...\n",
      "Processed 240 chunks...\n",
      "Processed 250 chunks...\n",
      "Processed 260 chunks...\n",
      "Processed 270 chunks...\n",
      "Processed 280 chunks...\n",
      "Processed 290 chunks...\n",
      "Processed 300 chunks...\n",
      "Processed 310 chunks...\n",
      "Processed 320 chunks...\n",
      "Processed 330 chunks...\n",
      "Processed 340 chunks...\n",
      "Processed 350 chunks...\n",
      "Processed 360 chunks...\n",
      "Processed 370 chunks...\n",
      "Processed 380 chunks...\n",
      "Processed 390 chunks...\n",
      "Processed 400 chunks...\n",
      "Processed 410 chunks...\n",
      "Processed 420 chunks...\n",
      "Processed 430 chunks...\n",
      "Processed 440 chunks...\n",
      "Processed 450 chunks...\n",
      "Processed 460 chunks...\n",
      "Processed 470 chunks...\n",
      "Processed 480 chunks...\n",
      "Processed 490 chunks...\n",
      "Processed 500 chunks...\n",
      "Processed 510 chunks...\n",
      "Processed 520 chunks...\n",
      "Processed 530 chunks...\n",
      "Processed 540 chunks...\n",
      "Processed 550 chunks...\n",
      "Processed 560 chunks...\n",
      "Processed 570 chunks...\n",
      "Processed 580 chunks...\n",
      "Processed 590 chunks...\n",
      "Processed 600 chunks...\n",
      "Processed 610 chunks...\n",
      "Processed 620 chunks...\n",
      "Processed 630 chunks...\n",
      "Processed 640 chunks...\n",
      "Processed 650 chunks...\n",
      "Processed 660 chunks...\n",
      "Processed 670 chunks...\n",
      "Processed 680 chunks...\n",
      "Processed 690 chunks...\n",
      "Processed 700 chunks...\n",
      "Processed 710 chunks...\n",
      "Processed 720 chunks...\n",
      "Processed 730 chunks...\n",
      "Processed 740 chunks...\n",
      "Processed 750 chunks...\n",
      "Processed 760 chunks...\n",
      "Processed 770 chunks...\n",
      "Processed 780 chunks...\n",
      "Processed 790 chunks...\n",
      "Processed 800 chunks...\n",
      "Processed 810 chunks...\n",
      "Processed 820 chunks...\n",
      "Processed 830 chunks...\n",
      "Processed 840 chunks...\n",
      "Processed 850 chunks...\n",
      "Processed 860 chunks...\n",
      "Processed 870 chunks...\n",
      "Processed 880 chunks...\n",
      "Processed 890 chunks...\n",
      "Processed 900 chunks...\n",
      "Processed 910 chunks...\n",
      "Processed 920 chunks...\n",
      "Processed 930 chunks...\n",
      "Processed 940 chunks...\n",
      "Processed 950 chunks...\n",
      "Processed 960 chunks...\n",
      "Processed 970 chunks...\n",
      "Processed 980 chunks...\n",
      "Processed 990 chunks...\n",
      "Processed 1000 chunks...\n",
      "Processed 1010 chunks...\n",
      "Processed 1020 chunks...\n",
      "Processed 1030 chunks...\n",
      "Processed 1040 chunks...\n",
      "Processed 1050 chunks...\n",
      "Processed 1060 chunks...\n",
      "Processed 1070 chunks...\n",
      "Processed 1080 chunks...\n",
      "Processed 1090 chunks...\n",
      "Processed 1100 chunks...\n",
      "Processed 1110 chunks...\n",
      "Processed 1120 chunks...\n",
      "Processed 1130 chunks...\n",
      "Processed 1140 chunks...\n",
      "Processed 1150 chunks...\n",
      "Processed 1160 chunks...\n",
      "Processed 1170 chunks...\n",
      "Processed 1180 chunks...\n",
      "Processed 1190 chunks...\n",
      "Processed 1200 chunks...\n",
      "Processed 1210 chunks...\n",
      "Processed 1220 chunks...\n",
      "Processed 1230 chunks...\n",
      "Processed 1240 chunks...\n",
      "Processed 1250 chunks...\n",
      "Processed 1260 chunks...\n",
      "Processed 1270 chunks...\n",
      "Processed 1280 chunks...\n",
      "Processed 1290 chunks...\n",
      "Processed 1300 chunks...\n",
      "Processed 1310 chunks...\n",
      "Processed 1320 chunks...\n",
      "Processed 1330 chunks...\n",
      "Processed 1340 chunks...\n",
      "Processed 1350 chunks...\n",
      "Processed 1360 chunks...\n",
      "Processed 1370 chunks...\n",
      "Processed 1380 chunks...\n",
      "Processed 1390 chunks...\n",
      "Processed 1400 chunks...\n",
      "Processed 1410 chunks...\n",
      "Processed 1420 chunks...\n",
      "Processed 1430 chunks...\n",
      "Processed 1440 chunks...\n",
      "Processed 1450 chunks...\n",
      "Processed 1460 chunks...\n",
      "Processed 1470 chunks...\n",
      "Processed 1480 chunks...\n",
      "Processed 1490 chunks...\n",
      "Processed 1500 chunks...\n",
      "Processed 1510 chunks...\n",
      "Processed 1520 chunks...\n",
      "Processed 1530 chunks...\n",
      "Processed 1540 chunks...\n",
      "Processed 1550 chunks...\n",
      "Processed 1560 chunks...\n",
      "Processed 1570 chunks...\n",
      "Processed 1580 chunks...\n",
      "Processed 1590 chunks...\n",
      "Processed 1600 chunks...\n",
      "Processed 1610 chunks...\n",
      "Processed 1620 chunks...\n",
      "Processed 1630 chunks...\n",
      "Processed 1640 chunks...\n",
      "Processed 1650 chunks...\n",
      "Processed 1660 chunks...\n",
      "Processed 1670 chunks...\n",
      "Processed 1680 chunks...\n",
      "Processed 1690 chunks...\n",
      "Processed 1700 chunks...\n",
      "Processed 1710 chunks...\n",
      "Processed 1720 chunks...\n",
      "Processed 1730 chunks...\n",
      "Processed 1740 chunks...\n",
      "Processed 1750 chunks...\n",
      "Indexing complete.\n",
      "User Query: Explain the concept of Retrieval-Augmented Generation.\n",
      "Executing RAG pipeline...\n",
      "Retrieving relevant chunks...\n",
      "Retrieved 5 chunks.\n",
      "Generating context from retrieved chunks...\n",
      "Formatting the prompt...\n",
      "Calling Llama API...\n",
      "Llama API call successful.\n",
      "Error calling Llama API: Extra data: line 2 column 1 (char 96)\n",
      "Q: Explain the concept of Retrieval-Augmented Generation.\n",
      "A: Error generating response.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configure paths\n",
    "DATA_DIR = \"./data\"  # Directory with PDF files\n",
    "LLAMA_API_URL = \"http://localhost:11434/api/generate\"  # Updated Llama API endpoint with correct port\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Load SentenceTransformer for embeddings\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize FAISS Index\n",
    "dimension = 384  # Embedding size for all-MiniLM-L6-v2\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Metadata store\n",
    "metadata_store = []\n",
    "\n",
    "### Step 1: Pre-Retrieval Optimization - Chunking ###\n",
    "def load_and_chunk_pdfs(data_dir, chunk_size, chunk_overlap):\n",
    "    \"\"\"\n",
    "    Load PDF files, extract text, and chunk them using a text splitter.\n",
    "    \"\"\"\n",
    "    print(\"Loading and chunking PDFs...\")\n",
    "    chunks = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            try:\n",
    "                reader = PdfReader(filepath)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text()\n",
    "                doc_chunks = splitter.split_text(text)\n",
    "                chunks.extend([(chunk, {\"source\": filename}) for chunk in doc_chunks])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "    \n",
    "    print(f\"Total chunks created: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "# Load and chunk PDFs\n",
    "chunked_data = load_and_chunk_pdfs(DATA_DIR, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "### Step 2: Pre-Retrieval Optimization - Embedding and Indexing ###\n",
    "def index_chunks(chunks, model, index, metadata_store):\n",
    "    \"\"\"\n",
    "    Generate embeddings for chunks and add them to the FAISS index along with metadata.\n",
    "    \"\"\"\n",
    "    print(\"Generating embeddings and indexing chunks...\")\n",
    "    embeddings = []\n",
    "    for idx, (chunk, metadata) in enumerate(chunks):\n",
    "        try:\n",
    "            embedding = model.encode(chunk)\n",
    "            embeddings.append(embedding)\n",
    "            metadata_store.append(metadata)\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Processed {idx} chunks...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding for chunk {idx}: {e}\")\n",
    "    index.add(np.array(embeddings).astype('float32'))\n",
    "    print(\"Indexing complete.\")\n",
    "\n",
    "# Index the chunks\n",
    "index_chunks(chunked_data, embedding_model, faiss_index, metadata_store)\n",
    "\n",
    "### Step 3: Retrieval Optimization ###\n",
    "def retrieve_relevant_chunks(query, model, index, metadata_store, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-k relevant chunks for a given query using FAISS index.\n",
    "    \"\"\"\n",
    "    print(\"Retrieving relevant chunks...\")\n",
    "    try:\n",
    "        query_embedding = model.encode(query)\n",
    "        distances, indices = index.search(np.array([query_embedding]).astype('float32'), top_k)\n",
    "        retrieved_chunks = [(chunked_data[i][0], metadata_store[i]) for i in indices[0]]\n",
    "        print(f\"Retrieved {len(retrieved_chunks)} chunks.\")\n",
    "        return retrieved_chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error during retrieval: {e}\")\n",
    "        return []\n",
    "\n",
    "### Step 4: Post-Retrieval Optimization - Context and Prompt Engineering ###\n",
    "def generate_context(retrieved_chunks):\n",
    "    \"\"\"\n",
    "    Generate a context string by concatenating retrieved chunks.\n",
    "    \"\"\"\n",
    "    print(\"Generating context from retrieved chunks...\")\n",
    "    return \" \".join([chunk for chunk, _ in retrieved_chunks])\n",
    "\n",
    "def format_prompt(context, query):\n",
    "    \"\"\"\n",
    "    Format the prompt with the retrieved context and user query.\n",
    "    \"\"\"\n",
    "    print(\"Formatting the prompt...\")\n",
    "    return f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Query:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "### Step 5: Generation with Llama ###\n",
    "def call_llama_api(prompt):\n",
    "    \"\"\"\n",
    "    Call the Llama model's API to generate a response based on the prompt.\n",
    "    \"\"\"\n",
    "    print(\"Calling Llama API...\")\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": \"llama2:7b\",  # Model name\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 256,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(LLAMA_API_URL, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        print(\"Llama API call successful.\")\n",
    "        return response.json()[\"choices\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Llama API: {e}\")\n",
    "        return \"Error generating response.\"\n",
    "\n",
    "### Step 6: RAG Pipeline ###\n",
    "def rag_pipeline(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline:\n",
    "    1. Retrieve relevant chunks.\n",
    "    2. Generate context.\n",
    "    3. Format prompt.\n",
    "    4. Generate response using Llama.\n",
    "    \"\"\"\n",
    "    print(\"Executing RAG pipeline...\")\n",
    "    # Retrieve relevant chunks\n",
    "    retrieved_chunks = retrieve_relevant_chunks(query, embedding_model, faiss_index, metadata_store, top_k)\n",
    "\n",
    "    if not retrieved_chunks:\n",
    "        return \"No relevant chunks retrieved.\"\n",
    "\n",
    "    # Generate context\n",
    "    context = generate_context(retrieved_chunks)\n",
    "\n",
    "    # Format prompt\n",
    "    prompt = format_prompt(context, query)\n",
    "\n",
    "    # Generate response using Llama\n",
    "    response = call_llama_api(prompt)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example Query\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"Explain the concept of Retrieval-Augmented Generation.\"\n",
    "    print(f\"User Query: {user_query}\")\n",
    "    answer = rag_pipeline(user_query)\n",
    "    print(f\"Q: {user_query}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7a451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe42700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9620a956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd428f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b2dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0073e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
